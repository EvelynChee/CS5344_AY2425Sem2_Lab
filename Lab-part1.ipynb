{"cells":[{"cell_type":"markdown","metadata":{"id":"ZE5FExCYe8ax"},"source":["# 0. Installing PySpark in Google Colab"]},{"cell_type":"markdown","metadata":{"id":"pei0I5IYf4xd"},"source":["Install Dependencies (needs to be done once each time you re-open this notebook):\n","\n","1.   Java 8\n","2.   Apache Spark with hadoop and\n","3.   Findspark (used to locate the spark in the system)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1cJjMt8Aeb9J"},"outputs":[],"source":["# install java\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","\n","# install spark (change the version number if needed)\n","!wget -q https://dlcdn.apache.org/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz\n","\n","# unzip the spark file to the current folder\n","!tar xf spark-3.5.3-bin-hadoop3.tgz\n","\n","# set your spark folder to your system path environment.\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.3-bin-hadoop3\"\n","\n","# install findspark using pip\n","!pip install -q findspark\n","import findspark\n","findspark.init()"]},{"cell_type":"markdown","metadata":{"id":"0eOTJa-R08yZ"},"source":["# 1. Wordcount"]},{"cell_type":"markdown","metadata":{"id":"CvO5VA9zcazU"},"source":["Setup up path to file before running if you are using Google Colab"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2668,"status":"ok","timestamp":1733194863787,"user":{"displayName":"Evelyn Chee","userId":"03637392660163232340"},"user_tz":-480},"id":"lu4BPD7p1zH-","outputId":"4a3c7606-fe56-45ed-8707-aeafa88e9da8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","./gdrive/MyDrive/CS5344_Lab/CS5344_AY2425Sem2_Lab/\n"]}],"source":["import sys, os\n","if 'google.colab' in sys.modules:\n","    from google.colab import drive\n","    drive.mount('/content/gdrive')\n","    # find automatically the path of the folder containing \"file_name\" :\n","    file_name = 'Lab-part1.ipynb'\n","    import subprocess\n","    path_to_file = subprocess.check_output('find . -type f -name ' + str(file_name), shell=True).decode(\"utf-8\")\n","    path_to_file = path_to_file.replace(file_name,\"\").replace('\\n',\"\")\n","    # if previous search failed or too long, comment the previous line and simply write down manually the path below :\n","    #path_to_file = '/content/gdrive/My Drive/CS5344_AY2425Sem2_Lab'\n","    print(path_to_file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_1vBqmUael9O"},"outputs":[],"source":["import re\n","from pyspark.sql import SparkSession\n","\n","# initalize SparkContext\n","spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n","sc = spark.sparkContext"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rwbVSXuXcazX"},"outputs":[],"source":["# read data from given input path\n","# you can try out using different input files\n","lines = sc.textFile(f'{path_to_file}/Lab-part1_in.txt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f1szDaqncazX"},"outputs":[],"source":["# flatMap converts \"list of list\" into a single list by concatenating them after the mapping function\n","# e.g. [\"First line text\", \"Second line text\"] -> [\"First\", \"line\", \"text\", \"Second\", \"line\", \"text\"]\n","words = lines.flatMap(lambda l: [word for word in re.split(r'\\W+', l.lower()) if word.isalpha()])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ILR7lr8HcazY"},"outputs":[],"source":["# pair each word with value 1\n","# e.g. [(\"First\", 1), (\"line\", 1), (\"text\", 1), (\"Second\", 1), (\"line\", 1), (\"text\", 1)]\n","pairs = words.map(lambda w: (w, 1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HUSXRWgYcazY"},"outputs":[],"source":["# reduceByKey group tuples by the first item and perform reduce operation on the second item\n","# e.g. [(\"First\", 1), (\"line\", 2), (\"text\", 2), (\"Second\", 1)]\n","counts = pairs.reduceByKey(lambda n1, n2: n1 + n2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2GkxdhW7cazY","executionInfo":{"status":"ok","timestamp":1733194881900,"user_tz":-480,"elapsed":4778,"user":{"displayName":"Evelyn Chee","userId":"03637392660163232340"}},"outputId":"68acbfa9-38c8-4fbf-be34-8f3bc8ba18c3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('are', 2),\n"," ('as', 8),\n"," ('look', 1),\n"," ('walk', 1),\n"," ('only', 1),\n"," ('love', 1),\n"," ('share', 1),\n"," ('people', 1),\n"," ('not', 1),\n"," ('beautiful', 2)]"]},"metadata":{},"execution_count":8}],"source":["# print out the first 10 outputs\n","counts.take(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3HlA0gCLcazY"},"outputs":[],"source":["# save data into file\n","# repartition(1) ensures the output are stored into a single file\n","counts.repartition(1).saveAsTextFile(f'{path_to_file}/Lab-part1_out')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TBE5w37icazZ"},"outputs":[],"source":["# stop the SparkContext\n","sc.stop()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iQHpjBZxjW9S"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}