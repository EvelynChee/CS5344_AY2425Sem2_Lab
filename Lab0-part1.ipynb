{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZE5FExCYe8ax"
   },
   "source": [
    "# 0. Installing PySpark in Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pei0I5IYf4xd"
   },
   "source": [
    "Install Dependencies (needs to be done once each time you re-open this notebook):\n",
    "\n",
    "1.   Java 8\n",
    "2.   Apache Spark with hadoop and\n",
    "3.   Findspark (used to locate the spark in the system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1cJjMt8Aeb9J"
   },
   "outputs": [],
   "source": [
    "# install java\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "\n",
    "# install spark (change the version number if needed)\n",
    "!wget -q https://dlcdn.apache.org/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz\n",
    "\n",
    "# unzip the spark file to the current folder\n",
    "!tar xf spark-3.5.3-bin-hadoop3.tgz\n",
    "\n",
    "# set your spark folder to your system path environment.\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.3-bin-hadoop3\"\n",
    "\n",
    "# install findspark using pip\n",
    "!pip install -q findspark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0eOTJa-R08yZ"
   },
   "source": [
    "# 1. Wordcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup up path to file before running if you are using Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 598,
     "status": "ok",
     "timestamp": 1732765210740,
     "user": {
      "displayName": "Evelyn Chee",
      "userId": "03637392660163232340"
     },
     "user_tz": -480
    },
    "id": "lu4BPD7p1zH-",
    "outputId": "c19ed4da-91bc-479a-eaa0-646632eee0ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./gdrive/MyDrive/CS5344_Lab/CS5344_AY2425Sem2_Lab/\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    # find automatically the path of the folder containing \"file_name\" :\n",
    "    file_name = 'Lab0-part1.ipynb'\n",
    "    import subprocess\n",
    "    path_to_file = subprocess.check_output('find . -type f -name ' + str(file_name), shell=True).decode(\"utf-8\")\n",
    "    path_to_file = path_to_file.replace(file_name,\"\").replace('\\n',\"\")\n",
    "    # if previous search failed or too long, comment the previous line and simply write down manually the path below :\n",
    "    #path_to_file = '/content/gdrive/My Drive/CS5344_AY2425Sem2_Lab'\n",
    "    print(path_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_1vBqmUael9O"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# initalize SparkContext\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from given input path\n",
    "# you can try out using different input files\n",
    "lines = sc.textFile(f'{path_to_file}/Lab0_part1_in.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatMap converts \"list of list\" into a single list by concatenating them after the mapping function\n",
    "# e.g. [\"First line text\", \"Second line text\"] -> [\"First\", \"line\", \"text\", \"Second\", \"line\", \"text\"]\n",
    "words = lines.flatMap(lambda l: re.split(r'[^\\w]+',l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair each word with value 1\n",
    "# e.g. [(\"First\", 1), (\"line\", 1), (\"text\", 1), (\"Second\", 1), (\"line\", 1), (\"text\", 1)]\n",
    "pairs = words.map(lambda w: (w, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduceByKey group tuples by the first item and perform reduce operation on the second item\n",
    "# e.g. [(\"First\", 1), (\"line\", 2), (\"text\", 2), (\"Second\", 1)]\n",
    "counts = pairs.reduceByKey(lambda n1, n2: n1 + n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the first 10 outputs\n",
    "counts.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data into file\n",
    "# repartition(1) ensures the output are stored into a single file\n",
    "counts.repartition(1).saveAsTextFile(f'{path_to_file}/Lab0_part1_out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the SparkContext\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iQHpjBZxjW9S"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMcKFz15xHzXCWRdrbvvRVG",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
